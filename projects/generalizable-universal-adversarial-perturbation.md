---
layout: post
title: Generalizable Universal Adversarial perturbations
thumbnail: img/fff.png
buttons:
  - CODE: https://github.com/BardOfCodes/G-UAP
  - PDF: ../../pdf/G_UAP.pdf
category: Stochastic
excerpt: This is a Paper we submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence, where we exploit the interesting phenomena of existence of universal adversarial perturbations(UAPs). We propose an algorithm for generating UAPs for any deep learning task, with or without the target data.
type: project
date: 2017/12/20
featured: true
anchors:
  - Introduction: introduction
  - Introduction to Universal Adversarial Perturbations: introduction-to-universal-advesarial-perturbations
  - The Algorithm: the-algorithm
  - Results: results
  - Links: links
---
### Introduction

During my Project Assistance-ship at Video Analytics Lab, IISc, I became fascinated with a very interesting aspect of deep learning. While the models generated from deep learning architectures performed amazingly well in almost all Computer Vision task, giving state-of-the-art performance on many metrics and datasets, they seemed to remain surprising defenseless against small engineered noises in the input. We add an engineered noise the the input, such that the change in the image is imperceptible to the human eye. The changed input has drastically poor predictions, where a classification model may predict a dog for a car, a segmentation model might label all pixels as sofa,etc.

This phenomena in my perspective presents the inherant weakness of our current learning paradigms. On the casual reasoning perspective, it seems that the models are not properly learning the hirarchical features and their combination, which is the general assumption. From a mathematical perspective, It seems to be a functional mapping whose output vary highly with minor changes in input(i.e. unbounded gradients w.r.t. input).

Is all of this just fun-side-projects and minor intricacies of deep learning? In my opinion, it is actually one of the biggest hurdles that the Deep learning "revolution" currently faces. When it comes to deploying a deep learning model for making critical decisions, that fact that such minor noises can drastically change its outcome may just be the deal breaker. Infrasturcture using deep networks might be highly susceptible to such attacks from hackers, where only a small noise has to be injected into the system to cause it to disfunction.

Hence, I decided that it is a crucial and impactful research topic, and took a deep dive into this area. In the next section I will describe what teh work in this paper. Link to code, paper are present in the bottom!



### Introduction to Universal Adversarial Perturbations

<img src ="../../img/fff.png" style="margin-left:auto;margin-right:auto;" />

Many Adversarial attacks algorithms like, FGSM\[1\], DeepFool\[2\], etc. have been proposed previously, which generate perturbations for each image, which leads the model to predict wrong information(like class, segmentation etc). While these perturbations are very effective, they generate an image-specific perturbation which is adversarila to that specific image only. A natural question that arises is can there be a global perturbation which can act as adversarial to the network independant of the input image? That is, can we generate a imperceptible perturbation that is adversarial for all(almost) images that are given to the network?

This kind of perturbation is called a Universal Adversarial Perturbation, introduced in [3]. In the paper, they propose a method to generate a Adversarial perturbation from 10.000 Image samples, which leads to spectacular drop in the performance of the model. They show the performance of their Adversarial perturbation on various CNNs for Classification. including big models such as ResNet152 as well as small models such as Caffenet. The metric used is Fooling Rate, which is the % of images for which the labels were changed after adding the pertubation.Their results are listed in Table 1.

The paper establised the existence of Universal Adversarial Perturbations. Now, that the existence is established, a natural progression of reasoning leads to the question that can a UAP be crafted with no use of data? Do we necessarily need 10000 Images? 

In the work that preceeds this publication, authors, Mopuri Konda Reddy and Utsav Garg found out that such a perturbation can be crafted even in the absence of data! In a paper titled "Fast-Feature-Fool" \[4\], they introduced a new algorithm to form UAPs without use of any images. While the fooling rateis lower to the UAP generated by Seyed-Mohsen Moosavi-Dezfooli et al, it shows some other interesting properties, such as being trasferable across dataset, being magnitudes faster to generate etc. 

The paper I worked on is a Journal extension of this work. While Journals generally do not contain substantial amount of additional work than a published conference paper(personal opinion), this work has substantial addition which make "fast-feature-fool" a much more competetive algorithm, and enable advarsarial generation on any task,from Classification to Depth Estimation. 

Tha major Contributions of this work are:

1) Significant enhancement in the algorithm, leading to better performing UAPs.

2) Introducing the concept of using data priors for generating UAPs, which makes the UAP as strong as the ones introduced in [3].

3) Show the generalizability of the algorithm proposed, by showing evidence of 'Fooling' on various tasks like Classification, Semantic Segmentation, and depth estimation.
 
### Results

Here we present some numbers from the paper, which should intrigue you enough to have a look at the paper itself:

$$
\subsubsection{Fooling performance of the data-free objective}
\begin{table}[]
\caption{Fooling rates for the proposed data-free objective. Diagonal rates indicate the white-box attacking and off-diagonal ones represent the black-box attacking scenarios.}
\centering
\label{tab:fooling-transfer}
\begin{tabular}{@{}lccccccc@{}}
%\small
\toprule
Model        & Caffenet       & VGG-F          & GoogLeNet      & VGG-16         & VGG-19         & Resnet-152     \\ \midrule
Caffenet     & \textbf{87.02} & 65.97          & 49.40          & 50.46          & 49.92          & 38.57          \\
VGG-F        & 59.89          & \textbf{91.91} & 52.24          & 51.65          & 50.63          & 40.72         \\
GoogLeNet    & 44.70          & 46.09          & \textbf{71.44} & 37.95          & 37.90          & 34.56         \\
VGG-16       & 50.05          & 55.66          & 46.59          & \textbf{63.08} & 56.04          & 36.84         \\
VGG-19       & 49.11          & 53.45          & 40.90          & 55.73          & \textbf{64.67} & 35.81         \\
Resnet-152   & 38.41          & 37.20          & 33.22          & 27.76          & 26.52          & \textbf{37.3} \\ \bottomrule
\end{tabular}
\end{table}
$$

Add table for Segmentation and Depth Estimation.



### References

\[1\] : Explaining and harnessing adversarial examples; Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.

\[2\] : DeepFool: a simple and accurate method to fool deep neural networks; Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard.

\[3\] : Universal adversarial perturbations; Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal Frossard.

\[4\] : Fast Feature Fool: A data independent approach to universal adversarial perturbations; Konda Reddy Mopuri, Utsav Garg, R. Venkatesh Babu

### Links

The code for this paper has been written in python on Pytorch as well as Tensorflow.You can,

* Download the code [here](https://github.com/BardOfCodes/G-UAP)

* Download the Paper [here](../../pdf/G_UAP.pdf).
